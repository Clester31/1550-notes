# Lecture 13

## Review

### Motivating our Scheduler

* CPU vs I/O Bound
  * Wall-clock time: How long it seems that a program takes to execute
  * User time: How long the user executes instructions on behalf of the process
  * System time: How long the OS was doing instructions on behalf of the process
* CPU bound and IO processes both take 1 unit of time
* When we block, we use that time while waiting to run instructions in another process
  * Allows us to use less time running multiple processes
 
## Characterizing our Workload
 
* Multiple I/O processes can be scheduled at the same time without increasing the amount of time necessary to run them
  * I/O bound processes are very common in programs
 
*  Computational bursts before the process blocks might be so long, that the illusion of the animation will break
*  If the program doesn't naturally produce an event, we preempt via an preemption timer that runs an interrupt after a certain time
  * We have preemption events that are split up sections of the CPU bound time to run CPU bursts
    * In a batch system, this means that our process is slower, but it retains the illusion of doing two things at once 
  * When the timer goes off, go through the interrupt vector, we have a context switch and move into the Kernel
  * The amount of time before preemtion is long enough so that we have time to block
    * This prevents a situation where we would have two context switches because of the preemption and the block
* Every preemption interrcupt is overhead needed to maintain the illusion of two things running at the same time

* How are most processes I/O bound?
  * Although our process are interractive, our users are much slower compared to the processor
    * Most of the programs we user are I/O bound because we interact with them
* Turning a CPU bound process into an I/O bound process
  * The only instance where something will take half time time is if something is entirely CPU bound, since it only has to worry about computations.
  * When the CPU gets faster, the bursts of computation keep getting smaller until the program becomes virtually entirely CPU bound
  * Having more transistors translates into faster processes beccause the CPU components are closer physically, reducing the time it takes for instructions to reach their destination
  * More complicated CPU's allows for different algorithm implementations, leading to better performance
 
## Three-Level Scheduling

![image](https://github.com/Clester31/1550-notes/assets/91839534/14292d9d-dff2-4d5f-b641-6d7b8dde19fc)

* What else could be considered scheduling
  * CPU scheduler is the only one that can be deemed postitive (picks what process to run)
  * Other places we can schedule (negative scheduling - Denies something the CPU, i.e. Don't give it RAM)
    * Admission Scheduler
      * Ask it if we should load a process in to RAM
      * If it says no (not in/enough RAM), then the CPU scheduler cannot pick it
      * Or, our workload added into RAM will not be better off with more work (no performance gain)
    * Memory Scheduler
      * Evict the parts we arent using to free up RAM 
      * A place where we take a process with partial execution in RAM and temporarily hold it somewhere else (like a disk)
     
### Scheduling Metrics

* Quantitative Metrics
  * Throughput - Number of jobs completed per unit time
  * Turnaround time - The time between a job being completed and submitted (not the time the job started and ended; that's wall clock)
    * Includes time its waiting to be scheduled 
  * Average turnaround time - the average of the turnaround times of a workload
 
* Computer Science Metrics
  * Asymptotic runtime - Big O for the scheduling algorithm/data structure maintenance
  * Engineering difficulty - How hard is it to get a particular algorithm right

#### Fairness

* **Comparable processes get comparable service**
  
* Comparable does not mean identical/equal
  * How can two processes run at the same time on a single CPU? 
  
