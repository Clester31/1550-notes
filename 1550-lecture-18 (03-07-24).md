# Lecture 15

## Review

![image](https://github.com/Clester31/1550-notes/assets/91839534/ae56c420-d26d-4892-807b-029c868778db)

* We can do parallel work concurrently by sharing a data structure (queue) to communicate
  * An acceptable outcome would be that both ouf our data would end up in the queue, regardless of order
  * Sometiems, we have an interrupt that skips the tail enqueue that overwrites what we had at the tail initially
    * We could run process A that would insert 9 at the tail, then run B before we enqueue and replace 9 with 20
   
### Queue with a race condition

* We need a global (shared) state and two or more processes that are using it
* We also have to have some work done in our process that is non-atomic (it can be interrupted)
* Our work also has to be doing two things
  * It has to be able to read that shared state
  * Then we have to "act" based on what we read
    * We can have an if statement based on what we read to make a decision 
    * We could have an instance where the state could change out from under us
   
* Race conditions can be latent (invisible)
  * We could run this code for a long time and potentially never see a buf
  * In order to see the RC manifest itself as a bug
    * We have to be interrupted in the middle of the code (unlikely)
    * We interleave two pieces of code that deal with the enqueing process
  * Why care?
    * We could accidentally modify another process that will can causes it to malfunction
    * Things that aren't our code could change, and something that used to work, wont work anymore
   
### Race Conditions

* When a process or thread has:
  * Shared global state
  * A Multiple instruction (non-atomic) operation where:
    * State is read then
    * State is acted upon (could be a write or something else)
  * Between reading and acting upon the data, the data could be changed out from under us, rendering our action wrong

* Race conditions can be latent.
  ◦ We only see the bug manifest itself when we are “unlucky”
* Fix with synchronization

* Imagine we have process A
  * We add 1 to 3
  * Now we interrupr and go into process be, **Do a context switch**
  * Now we are in process B, we see the value is 4 and add 1 to it
  * We then go back into A, and we fulfill our context switch, returning the original vlaues
    * 3 + 1 + 1 = ...4?
   
#### How to Fix?

* Use two operations to define a **Critical Region** (critical section):

```C
// To go before the first instruction in the critical region
enter_critical_region() {

}
// To go after the last instruction in the critical region
leave_critical_region() {

}
```

* How do we define this critical region?
  * We define something generically called enter_critical_region() and one called leave_critical_region()
 
* What if we just blame the interruption? No interrupt, means our race condition issue is solved... right?

#### Blame Preemption

```C
// To go before the first instruction in the critical region
enter_critical_region() {
  disable_interrupts();
}
// To go after the last instruction in the critical region
leave_critical_region() {
  reenable_interrupts();
}
```

* Problems:
  * The enforcement of our OS as the manager of resources is basically thrown away if we disable interrupts
  * Parallelism no longer exists as we can no longer interleave processes, which are a product of interrupts
 
### What We Really Want

![image](https://github.com/Clester31/1550-notes/assets/91839534/f71b1970-1192-498c-ab7c-a12bacc9f0e0)

* Enter into our critical region of A
* If B tries to call enter_critical_region() while A is doing work in the critical region, we block B
* Once A calles leave_critical_region(), we can enter the critical region of B

* We really don't want two critical regions to overlap at the same time
* We want one process to be execution code in a critical region to the exclusion of all the others

### Mutual Exclusion

* The mutex is a shared region

![image](https://github.com/Clester31/1550-notes/assets/91839534/ad11b33d-ef93-407f-9aa1-b77661f7d261)

* We have a lock that unlocks when we leave the mutex and locks when we enter it
* A runs first, we lock the Mutex
* We enter B, we try to go into the critical region, but our Mutex is blocked
* "If you try to lock a locked lock, you block"
* What now? we still have to do work
  * Just run another process
* Process A leaves the critical region, unlocks the mutex, and then we do the work in B's critical region

#### Synchronization in pthreads

```C
#include <stdio.h>
#include <pthread.h>

int tail = 0;
int A[20];
pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;

void enqueue(int value) {
  pthread_mutex_lock(&mutex);
  A[tail] = value;
  tail++;
  pthread_mutex_unlock(&mutex);
}
```

### Goals for Synchronization

1. No two processes can be inside their critical regions at the same time
  * It needs to work 
2. No assumptions about CPU or number of CPUs
  * It should work on any system
3. No process outside its critical region may block another process
  * If you're outside a critical region, their is no race condition to worry about
4. No process should have to wait forever to enter its critical region
  * Because of the implementation of enter/leave critical region
  * Note that if the work of the critical region is infinte, ecause of goal 1, we won't be able to stop it

#### Busy waiting

* Waiting but we are doing useless stuff
* While inneficieint, it prevents enter_critical_region from returning (blocked)



